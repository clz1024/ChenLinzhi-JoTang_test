{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5163a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 相同文件重命名\n",
    "def rename_duplicate(list, print_result=False):\n",
    "    new_list=[v + str(list[:i].count(v) + 1) if list.count(v) > 1 else v for i, v in enumerate(list)]\n",
    "    if print_result:\n",
    "\t    print(\"Renamed list:\",new_list)\n",
    "    return new_list\n",
    "\n",
    "# 1 拿到所有的新闻网址\n",
    "page = 1\n",
    "page_content = \"\"\n",
    "url_list2 = []\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'\n",
    "}\n",
    "while page < 6:\n",
    "    if page == 1:\n",
    "        url = \"https://sise.uestc.edu.cn/xwtz/tzgg/yb.htm\"\n",
    "    else:\n",
    "        code = 6-page\n",
    "        url = f\"https://sise.uestc.edu.cn/xwtz/tzgg/yb/{code}.htm\"\n",
    "    resp = requests.get(url)\n",
    "    resp.encoding = \"UTF-8\"\n",
    "    # session = requests.Session()\n",
    "    # resp = session.get(url=url, headers=headers).text.encode('latin1')\n",
    "    page_content += resp.text\n",
    "    # print(resp.text)\n",
    "\n",
    "    obj1 = re.compile(r'/system/resource/js/ajax.js\">(?P<news>.*?)<link rel=\"stylesheet\" Content-type=\"text/css\" ', re.S)\n",
    "    result1 = obj1.finditer(page_content)\n",
    "    # print(result1)\n",
    "    for it in result1:\n",
    "        news = it.group('news')\n",
    "        # print(ul)\n",
    "        if page == 1:\n",
    "            obj2 = re.compile(r'<a href=\"../../(?P<url>.*?)\">', re.S)\n",
    "        else:\n",
    "            obj2 = re.compile(r'<a href=\"../../../(?P<url>.*?)\">', re.S)\n",
    "        result2 = obj2.finditer(news)\n",
    "        # print(result2)\n",
    "        for itt in result2:\n",
    "            url ='https://sise.uestc.edu.cn/' + itt.group('url')\n",
    "            # print(url)\n",
    "            url_list2.append(url)\n",
    "\n",
    "    page += 1\n",
    "    # 去重\n",
    "    url_list = []\n",
    "    for i in url_list2:\n",
    "        if not i in url_list:\n",
    "            url_list.append(i)\n",
    "\n",
    "# 2 拿到标题，创建每个新闻对应的文件夹\n",
    "    titles = []\n",
    "    for r in url_list:\n",
    "        # print(r)\n",
    "        resp = requests.get(r)\n",
    "        resp.encoding = \"UTF-8\"\n",
    "        child_page_content = resp.text\n",
    "        # print(child_page_content)\n",
    "        obj3 = re.compile(r'<title>(?P<titles>.*?)-', re.S)\n",
    "        result3 = obj3.finditer(child_page_content)\n",
    "\n",
    "        for title in result3:\n",
    "            title = title.group('titles')\n",
    "            # print(title)\n",
    "            titles.append(title)\n",
    "\n",
    "    renamed_titles = rename_duplicate(titles, False)\n",
    "\n",
    "    for tt in renamed_titles:\n",
    "        # print(tt)\n",
    "        os.mkdir(f'D:/Anaconda3/软网爬虫/任务一/{tt}')\n",
    "\n",
    "\n",
    "# 3 爬取标题 文本 时间放入文件夹\n",
    "\n",
    "code = 0\n",
    "for r in url_list:\n",
    "    # print(r)\n",
    "    txt_path = f'D:/Anaconda3/软网爬虫/任务一/{renamed_titles[code]}/标题，时间，文本.txt'\n",
    "    code += 1\n",
    "    f = open(txt_path, 'a', encoding='utf-8')\n",
    "\n",
    "    resp = requests.get(r)\n",
    "    resp.encoding = \"UTF-8\"\n",
    "    child_page_content = resp.text\n",
    "    obj3 = re.compile(r'<title>(?P<titles>.*?)-', re.S)\n",
    "    result3 = obj3.finditer(child_page_content)\n",
    "\n",
    "    for title in result3:\n",
    "        title = title.group('titles')\n",
    "        # print(title)\n",
    "        f.write(title+'   ')\n",
    "    obj4 = re.compile(r'<p class=\"content-tip\"> (?P<times>.*?) 作者', re.S)\n",
    "    result4 = obj4.finditer(child_page_content)\n",
    "    for time in result4:\n",
    "        time = time.group('times')\n",
    "        # print(time)\n",
    "        f.write(time+'\\n')\n",
    "    session = requests.Session()\n",
    "    resp = session.get(url=r, headers=headers).text.encode('latin1')\n",
    "    tree = etree.HTML(resp)\n",
    "    links = tree.xpath(\n",
    "        '/html/body/div[@class=\"news-list-content\"]/div[2]/div[2]/div[1]/div/p|//*[@id=\"vsb_content\"]/div/table|/html/body/div[2]/div[2]/div[2]/div[1]/div/form'\n",
    "         )\n",
    "    if not links:\n",
    "        f.write('无正文')\n",
    "        break\n",
    "    for li in links:\n",
    "        page_list_li = li.xpath('.//text()')\n",
    "        f.writelines(page_list_li)\n",
    "\n",
    "f.close()\n",
    "print('TXT OVER!')\n",
    "\n",
    "# 4 爬取图片放入相应文件夹\n",
    "code = 0\n",
    "\n",
    "for r in url_list:\n",
    "\n",
    "    resp = requests.get(r)\n",
    "    resp.encoding = \"UTF-8\"\n",
    "    child_page_content = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    img_list = child_page_content.find(\"div\", class_=\"v_news_content\").find_all(\"img\")\n",
    "    # print(img_list)\n",
    "\n",
    "    # 处理重复文件名\n",
    "    img_list = rename_duplicate(str(img_list), False)\n",
    "\n",
    "    for li in img_list:\n",
    "        obj5 = re.compile(r'<img src=\"(?P<src>.*?)\"/>',re.S)\n",
    "        result5 = obj5.search(li)\n",
    "        if not result5:\n",
    "            break\n",
    "        src = result5.group('src')\n",
    "\n",
    "        img_url = \"https://sise.uestc.edu.cn\" + src\n",
    "        img = requests.get(img_url)\n",
    "        # print(img)\n",
    "        img_name = src.split(\"/\")[-1]\n",
    "        img_name = img_name.split('?')[0]\n",
    "        # print(img_name)\n",
    "\n",
    "        img_path = f'D:/Anaconda3/软网爬虫/任务一/{renamed_titles[code]}/{renamed_titles[code]}' + img_name\n",
    "        code += 1\n",
    "        with open(img_path, mode=\"wb\") as img_f:\n",
    "            img_f.write(img.content)\n",
    "\n",
    "print('IMG OVER!')\n",
    "\n",
    "\n",
    "# 5 爬取附件放入相应文件夹\n",
    "code = 0\n",
    "\n",
    "for r in url_list:\n",
    "    resp = requests.get(r)\n",
    "    resp.encoding = \"UTF-8\"\n",
    "    child_page_content = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    href_list = child_page_content.find(\"div\", class_=\"v_news_content\").find_all(\"a\")\n",
    "\n",
    "    for lii in href_list:\n",
    "        href = lii.get('href')\n",
    "        # print(href)\n",
    "\n",
    "        # 判断是否为空\n",
    "        if not href:\n",
    "            break\n",
    "        \n",
    "        # 区别不同网址\n",
    "        # try:\n",
    "        text1 = href.split(\".\")[0]\n",
    "        # except AttributeError:\n",
    "            # print(href)\n",
    "            # break\n",
    "        if text1 == 'https://www':\n",
    "            href_url = href\n",
    "        else:\n",
    "            href_url = \"https://sise.uestc.edu.cn\" + href\n",
    "\n",
    "        # 去除杂质\n",
    "        text2 = href + '@'\n",
    "        text2 = text2.split(\"@\")[1]\n",
    "        if text2 == 'uestc.edu.cn':\n",
    "            break\n",
    "        text3 = href.split(\".\")[0]\n",
    "        if text3 == 'http://uestc' or text3 == 'http://sose':\n",
    "            break\n",
    "\n",
    "        href_name = href.split(\"/\")[-1]\n",
    "        href_name = href_name.split('?')[0]\n",
    "        # print(href_name)\n",
    "        fail_name = \"ERROR \" + href_name\n",
    "        href_path = f'D:/Anaconda3/软网爬虫/任务一/{renamed_titles[code]}/' + href_name\n",
    "        fail_path = f'D:/Anaconda3/软网爬虫/任务一/{renamed_titles[code]}/' + fail_name + '.txt'\n",
    "        code += 1\n",
    "\n",
    "        try:\n",
    "            href_content = requests.get(href_url).content\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            with open(fail_path, mode=\"w\") as h_f:\n",
    "                h_f.write('附件失效')\n",
    "        else:\n",
    "            with open(href_path, mode=\"wb\") as h_f:\n",
    "                h_f.write(href_content)\n",
    "\n",
    "print('HREF OVER!')\n",
    "\n",
    "print('ALL OVER!!!!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
